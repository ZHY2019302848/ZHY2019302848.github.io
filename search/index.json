[{"content":"多目标追踪(Multi-Object-Tracking) 研究背景及意义 多目标跟踪(Multiple object tracking，MOT)的主要任务是从给定视频中输出所有目标的运动轨迹，并维持各目标的身份信息(Identity，ID)。其中，跟踪目标可以是行人、车辆或其他对象。 视频监控系统的普及： 随着视频监控系统在公共场所、交通系统、工业领域等的广泛应用，需要自动化地对视频中的多个目标进行实时监测和追踪。 自动驾驶技术的发展： 在自动驾驶领域，多目标追踪对于车辆和行人的准确追踪至关重要，以确保车辆能够安全行驶并适应复杂的交通环境。 智能交通系统的需求： 对于城市交通管理和规划来说，多目标追踪能够提供实时的交通流量信息、行人流动模式等，帮助改善交通系统的效率和安全性。 人机交互与辅助技术： 在智能家居、虚拟现实等领域，多目标追踪有助于提供更智能的人机交互和环境感知。 人工智能与机器学习： 多目标追踪是人工智能和机器学习领域的一个典型问题，解决这一问题涉及到对图像处理、目标检测、时空建模等多个方面的研究，有助于推动计算机视觉和机器学习的发展。 研究难点 然而，在复杂场景下进行鲁棒跟踪仍是当前的研究难点，主要体现在以下３个方面：\n跟踪过程中频繁遮挡导致目标难以被精准定位； 不同目标之间可能具有较高的外观相似性，增加了维持目标ID的难度； 目标间交互可能造成跟踪框漂移。 经典方法 MOT算法分类 基于SDE(Separate Detectionand Embedding)范式的方法 离线跟踪算法 离线跟踪可以看成是一个全局优化问题，给定所有视频帧的检测结果，将属于同一目标的检测结果全局关联到一条轨迹中。\n离线跟踪的关键是找到全局最优解。连续能量最小化是一种常用的全局优化方法，旨在将数据关联和轨迹估计整合到能量函数中，并通过构建运动模型来约束轨迹。另一种常用的全局优化策略是将MOT任务建模为一个图模型，其中每个顶点表示一个检测目标，顶点间的边表示目标间的相似性，然后通过匈牙利算法或贪婪算法确定各顶点的匹配关系。基于图模型的方法有网络流(Network Flow，NF)、条件随机场(CRF)、最小代价子图多切(Minimum Cost Subgraph Multicut，MC SM)和最大加权独立集(Maximum-Weight Independent Set，MWIS)等。\n由于在跟踪过程中可利用更多帧图像的信息，离线方法通常比在线方法具有更高的跟踪准确性和鲁棒性，但其计算量开销更高且实际应用范围相比在线方法较小。\n在线跟踪算法 由于在线跟踪方法具有不依靠未来信息的特点，更契合实际需求，因此在线的跟踪算法成为如今的研究主流。在线跟踪方法通常按时间顺序逐帧关联目标，因此在线跟踪也被称为顺序跟踪。当前的在线跟踪方法常基于目标的运动和外观特征关联目标。早期的研究主要通过构建运动模型，基于目标的运动特征来跟踪目标。随后，受益于神经网络强大的特征提取能力，基于外观特征的跟踪算法吸引了广泛的关注。而为了进一步提升算法在各种复杂的场景下的跟踪准确性，结合运动和外观特征的MOT算法成为了当今的研究热点。\n可以分为三种（做一个图，后面跟上这些方法的优缺点）\n基于运动特征的方法 优点：有效应对短时间的遮挡且缓解了相似目标对模型的干扰 缺点：由于外观特征的缺失，跟踪性能衰退明显 基于外观特征的方法 优点：跟踪能力更强，对目标尺度变换的鲁棒性更高 缺点：有相似目标干扰的场景下容易发生跟踪框漂移等错误 结合运动和外观特征的方法 优点：跟踪准确性较高，应对复杂场景下的各种挑战具有更强的鲁棒性 缺点：网络复杂度较高且计算量相对较大，跟踪速度较慢，难以达到实时跟踪的要求 基于JDE(Joint Detection and Embedding)范式的方法 SDE的方法在跟踪过程中先后推理了目标检测和特征提取两个计算量较大的深度网络，这种高昂的计算开销限制了模型的跟踪速度，通过使目标检测和特征提取两个关键任务共享大量特征，即将两个任务融合到一个网格中去，JDE范式可以显著减少算法的计算量。 优化的具体三点\n联合检测和嵌入 JDE采用联合检测和嵌入的方式，通过一个统一的深度学习模型，同时执行目标检测和目标嵌入。 JDE基于深度卷积神经网络（CNN），包含目标检测分支和目标嵌入分支，两者共享底层卷积层，以促使特征提取得到充分共享和协同训练。 训练时采用联合训练策略，同时优化目标检测分支和目标嵌入分支的损失函数。损失函数包括目标检测的定位损失、分类损失，以及目标嵌入的三元损失等，平衡了目标检测和嵌入学习两个任务。 基于JDT(Joint Detectionand Tracking)范式的方法 联合检测和跟踪在单个网格中完成三个子任务进行算法优化 当前JDT范式的算法主要分为基于孪生网路的方法和Transformer的方法 基于孪生网络的方法 基本原理：孪生网络是标准CNN的一种变体。如图所示，基于孪生网络的方法通过两个共享权重的卷积层提取不同视频帧图像中目标的特征，结合不同图像信息学习目标更具判别性的特征。 训练过程: 在训练孪生网络时，通常使用孪生三元损失（Siamese Triplet Loss）来优化网络参数。损失函数包括一个锚定样本、一个正样本和一个负样本，通过最小化锚定样本与正样本的距离同时最大化锚定样本与负样本的距离，使得相似的目标在嵌入空间中更加接近。 目标跟踪过程：在跟踪时，通过比较当前帧目标图像和前一帧目标图像的特征表示相似性，判断目标是否相同。一些孪生网络方法还采用在线学习的策略，通过不断地更新孪生网络的权重，适应目标外观的变化，提高跟踪的鲁棒性。 Transformer方法 基本原理:Transformer是一种基于自注意力机制的神经网络结构，广泛应用于序列建模任务。在目标跟踪中，可以将目标的位置信息作为序列输入，并使用Transformer模型来捕捉目标之间的关系。 自注意力机制：Transformer使用自注意力机制来动态地捕获输入序列中各元素之间的关系，使得模型能够关注不同位置的信息，这其中的输入为目标的位置信息和特征，将二者嵌入组成序列。 目标跟踪过程：Transformer可以输出每个目标的位置信息以及其他可能的特征，通过比较输出信息的相似性进行目标匹配，能够更好地处理目标之间的复杂关系和变化 目标追踪方法 论文复现 总结 近年来，基于深度学习的MOT技术迅速发展，模型的跟踪性能取得了显著的提升，目前已有越来越多的技术被应用到MOT任务上，但目前还有许多值得探索的研究方向。\n无监督MOT：当前的MOT算法大多是基于监督学习，然而MOT数据集的标注需要逐帧寻找不同图像间的相同目标，需花费巨大的时间和经济成本。设计基于无监督学习的MOT算法有助于减少人工标注数据的开销，然而由于缺乏对跟踪目标的先验知识，无监督MOT任务具有很大的挑战性。 目标间交互关系：通过对多个目标间的交互关系建模，可增强拥挤场景下模型对各目标的判别能力，然而当前算法对于目标间交互关系的探索依然较少。在今后的研究工作中，可采用Transformer或图神经网络对目标间的交互关系进行建模，从而进一步提升MOT算法在高峰时段的地铁站和节假日的旅游景点等极端拥挤场景下的跟踪鲁棒性。 跟踪促进检测：当前的MOT算法跟踪性能依赖于检测算法，然而目前的MOT算法通常单独执行检测算法，并未探索目标在先前时刻的信息。充分利用目标的时空信息，将目标在过去时刻的运动和外观等特征传递到当前帧，有助于提升模型在执行交通车辆跟踪和赛场运动员行为分析等存在大量遮挡和运动模糊的跟踪任务时的跟踪性能。 ","date":"2023-11-23T18:31:02+08:00","permalink":"https://ZHY2019302848.github.io/p/mot/","title":"MOT"},{"content":"Wonder3D环境配置 项目仓库\n项目论文\n准备系统环境 系统为ubuntu18.04以上系统，anaconda环境装好，cuda版本为11.8或11.7，测试用12.2会报错，gcc版本8以上，python3.8以上，显卡确保显存足够，我先后在2080ti和titan显卡上均测试过，cuda版本为11.3和11.2，都会报cuda和pytorch版本不匹配的错，最后在4090上成功运行，所以一个准确合适的环境是重要前提。\n安装环境 只要上一步系统环境准备完好，接下来依照官方教程即可\n1 2 conda create -n wonder3d python=3.10 #这里用3.10是怕再有奇怪的报错，所以把版本调高 conda activate wonder3d 这两步结束后，先查看python和pip的路径是conda虚拟环境，用which python和which pip查询，如果返回时虚拟环境的路径，可以跳过这一步骤，如果不是 ，显示为local路径，可以先退出现在的base环境，然后在换回来wonder3d环境\n1 2 3 conda deactivate #退出wonder3d conda deactivate #退出base conda activate wonder3d #重新激活环境 下面是项目下载和必要环境的安装\n1 2 3 4 git clone https://github.com/xxlong0/Wonder3D #这里如果速度慢可以参考上一篇\u0026#34;服务器网络问题解决方案\u0026#34;，但这个只能用于大学教研室或是公司本地服务器 cd Wonder3D pip install -r requirements.txt #这要下很久，可以去泡杯茶 pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch #这里很容易出问题，大部分问题都可以在\u0026#34;准备系统环境\u0026#34;中解决 安装完后看一下自己torch是否安装成功\n1 python 1 2 3 import torch print(torch.__verison__)#查看版本号是否对应后退出即可 exit() 运行项目 先下载checkpoint文件复制到指定目录下\n1 2 3 4 5 Wonder3D |-- ckpts |-- unet |-- scheduler.bin ... 之后可以直接运行命令看一下效果，代码会默认跑一只猫头鹰\n1 bash run_test.sh 同理，此处出现错误请返回系统环境和安装环境这两个步骤查看是否出错，如果按照错误提示打补丁很可能越补窟窿越大 运行完成后可以在./outputs下查找相关文件，应该6个一一对应的法线图和色彩图\n之后生成mesh obj文件，这里我只试了官方的第一种方法\n1 2 3 4 cd ./instant-nsr-pl bash run.sh output_folder_path scene_name #第二条命令示例为: bash run.sh ../outputs/cropsize-192-cfg3.0 owl 生成完成后可以用ls命令找哪个文件夹是新生成的说明obj文件就存在哪了\n如果想要替换自己的图片，先把图片的背景去掉，在线网站就可以完成，然后去找/wonder/configs/mvdiffusion-joint-ortho-6views.yaml，修改如下内容即可 尝试其他图片 我用该方法尝试了下最近师姐让找的car图片，因为该项目训练时没有车的训练模型，所以图生3D的效果不是十分理想。\ncar1 car2 car3 总结 锻炼了自己的环境配置功力，还有在项目介绍看到可以使用正交相机来完成对obj色彩的显示，原文如下：\nOur generated normals and color images are defined in orthographic views, so the reconstructed mesh is also in orthographic camera space. If you use MeshLab to view the meshes\n这个可以帮到我最近在研究的对对房屋mesh涂上迷彩的问题。\n","date":"2023-11-15T19:37:07+08:00","image":"https://ZHY2019302848.github.io/p/%E5%AE%9E%E8%B7%B5/fig_teaser_hu5f338f1929158fc1dbd893b6f223493f_1271319_120x120_fill_box_smart1_3.png","permalink":"https://ZHY2019302848.github.io/p/%E5%AE%9E%E8%B7%B5/","title":"Wonder3D项目环境配置和运行"},{"content":"对服务器网络问题的总结 最近使用实验室linux服务器跑代码，对其中频繁出现的网络问题烦不胜烦，诸如git和pip问题层出不穷，其中的问题不是简单换一个清华源可以，根源性需要让服务器走中转代理，实现完全性翻墙（该方法仅对内网有用）。\n现在把其中常见的命令总结如下，方便以后快捷使用。\ngit的代理设置 先对clash进行设置\n1 2 3 4 5 6 git config --global http.proxy socks5 127.0.0.1:7890 #端口号参考clash git config --global https.proxy socks5 127.0.0.1:7890 #端口前面的ip地址参考开clash的主机ip git config --global https.proxy 127.0.0.1:7890 git config --global https.proxy 127.0.0.1:7890 如果报错提示不让写入修改，在最后加入--replace-all，代码如下\n1 2 git config --global https.proxy 127.0.0.1:7890 --replace-all git config --global https.proxy 127.0.0.1:7890 --replace-all 服务器全局设置 考虑到有些工具无法设置代理，所以使用全局代理，但这种方式在针对ICMP协议时会失效，因为使用的代理走的都是会话层，无法影响到ip报文解包的结果。\n代码如下\n1 2 3 export proxy=\u0026#34;http://127.0.0.1:7890\u0026#34; ##ip和端口地址按情况替换 export https_proxy=$proxy export http_proxy=$proxy 效果 可以让torch的安装速度从几百k涨到最高10mb/s。\n备注\n如果无法正常部署博客到github.io，请检查标题里是不是把草稿draft设置为了true，之后再运行如下命令\n1 2 hugo hugo --gc --minify --cleanDestinationDir ","date":"2023-11-10T15:34:23+08:00","image":"https://ZHY2019302848.github.io/p/linux-network/linux_network_hu0f0ae4cf8191179714ea2b2d64d558ac_81456_120x120_fill_q75_box_smart1.jpg","permalink":"https://ZHY2019302848.github.io/p/linux-network/","title":"服务器网络问题解决方案"}]