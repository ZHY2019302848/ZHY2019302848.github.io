[{"content":"ratyrix相机操作简述 一.硬件的连接以及网络的设置 先插好必要的线，一根电源线，一根网线（师兄说两根网线可以提高视频的帧率，但因为驱动有问题没有实现），插电源线的时候左扭右扭一下，切忌大力出奇迹\n因为所用主机为特制专用的德意志冯字辈12年RTX680_win7主机，其自身带有wifi蓝牙模块，所以没有插联网网线，工作中也没有必要使用联网功能，故不连wifi也不插联网网线。\n对于网络设备的相关参数设置，win+r-\u0026gt;输入control（win11自带的设置一坨，还是老家伙便捷好使），找到网络适配器，在插线的网口适配器右键属性找到ipv4的设置，ip地址设置为私有地址，比如169.254.0.23,掩码设置为255.255.0.0,其余部分留空就好，之后保存应用退出。\n二.软件的使用 软件与相机的连接 软件在打开时会自动检查相机是否连接，如果显示无相机，则重新拔插电源线，再检测一遍，如果还是没有检测到，不用怀疑是不是一里面的设置有问题，直接重启电脑，拔插电源，重启软件，这三者交替使用，总有一次能检测到。\n相机的具体使用 这部分涉及到大量图片部分，文字描述的效果有限，而德意志主机的网络条件很差，所以只能在主机上截图后用U 盘导出来再构思该部分。\n三.深度图的生成 软件实际上自带有深度图生成，但是其图像总归不是我们所需要的理想状态（完全是一堆乱七八糟五彩光线），所以在师兄的推荐下，尝试使用外部代码解决深度图的生成问题\n参考代码:PlenopticToolbox2.0\n按照步骤应该先编译一遍python包，输入命令python setup.py build_ext --inplace即可，之后会根据你所使用的系统生成一个cpython文件，windows是.pyd,linux是.so,之后记住把该文件复制粘贴到disparity文件夹中\n之后我们把下载好的图片，选择一张processed图,并把下载的图片文件里的xml文件重名名和你图片相同名字后，复制粘贴到代码目录下，运行以下代码得到深度图：\n1 python disparity_sample.py beer/Beers_Processed.xml -dmin 0 -dmax 10 -scene \u0026#39;real\u0026#39; -format png 后面的format参数可以改为titf\n","date":"2024-01-11T19:04:42+08:00","permalink":"https://ZHY2019302848.github.io/p/%E5%85%89%E5%9C%BA%E7%9B%B8%E6%9C%BA%E7%9A%84%E6%93%8D%E4%BD%9C%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/","title":"光场相机的操作以及相关知识"},{"content":"第一步，把输出模式设为 高级 ，这样允许你进行高级操作，降低局限性。\n录制设置 录制路径：这是你的视频文件保存位置，推荐选择大一点的磁盘，防止中断录制。\n生成没有空格的文件名：生成的文件将不带有空格，而是用_代替。（用于不支持带有空格的文件名的场景）\n录像格式：这个决定了你是视频将用什么格式封装，推荐的是.mkv，原因是它什么都能装。当然考虑到你的pr不支持mkv了，在主窗口点击文件，再点击录像转封装，拖入视频文件点转换封装即可转为pr支持的mp4；代价是文件略微变大~1mb。\n视频编码器：重点！\n总结：\nNVENC AV1 \u0026gt; NVENC HEVC \u0026gt; NVENC H.264 | H264 AMF | QuickSync H.264\nSVT-AV1 \u0026gt; AOM-AV1\nx264 最低优先\nx264 使用CPU进行h.264编码。在大部分情况，它的效果不如你的核显，关键是性能，如果你在录制过程中打开了高cpu占用的程序（例如大游戏），会编码过载导致录制的视频变卡顿，如果使用更低的预设则会导致编码质量不是很高，而且有时候还是会卡录制。如果你想有好的录制效果和希望cpu编码，考虑AV1编码器。\nNVIDIA NVENC AV1 （至少需要英伟达40系显卡）使用GPU进行AV1编码。它会占用你显卡的Video Encode引擎，和游戏一般占用的3D和copy引擎是分开的，在大部分情况不会卡录制。（官方说明相较于NVENC H.264，效率提升1.45x）\nNVIDIA NVENC HEVC （至少需要英伟达20系显卡）使用GPU进行hevc编码。它会占用你显卡的Video Encode引擎，和游戏一般占用的3D和copy引擎是分开的，在大部分情况不会卡录制，但是如果可以进行NVIDIA NVENC AV1 则不建议选择此编码器，因为AV1更有效率。（官方说明相较于NVENC H.264，效率提升1.15x）\nNVIDIA NVENC H.264 （至少需要英伟达显卡）使用GPU进行h.264编码。它会占用你显卡的Video Encode引擎，和游戏一般占用的3D和copy引擎是分开的，在大部分情况不会卡录制，但是如果可以进行NVIDIA NVENC HEVC 则不建议选择此编码器，因为hevc更有效率。\nAOM-AV1 使用CPU进行aom-AV1编码，需要强劲的cpu，否则编码质量不高，由此可得需要大量cpu，cpu使用量跌宕不平，由画面复杂度决定，并且和x264一样容易卡录制（如果你的cpu满载），所以推荐的是：后期使用ffmpeg压缩，代码如下，这个压缩效率高得多，不卡视频，缺点是速度慢。\n1 ffmpeg -i %1 -c:v libsvtAV1 -crf 40 -bf 4 -preset 5 -g 240 \u0026#34;.\\%~n1_SAV1.mp4\u0026#34; SVT-AV1 使用CPU进行svt-AV1编码，需要强劲的cpu，否则编码质量不高，由此可得需要大量cpu，cpu使用量跌宕不平，由画面复杂度决定，并且和x264一样容易卡录制（如果你的cpu满载），所以推荐的是：后期使用ffmpeg压缩，代码如下，这个压缩效率高得多，不卡视频，缺点是速度慢。 对于aom-AV1，它的效率更高。 1 ffmpeg -i %1 -c:v libsvtAV1 -crf 40 -bf 4 -preset 5 -g 240 \u0026#34;.\\%~n1_SAV1.mp4\u0026#34; H264 AMF （至少需要AMD显卡）使用GPU进行h.264编码。\nQuickSync H.264 （至少需要Intel显卡）使用GPU进行h.264编码。 如果你有核显，又有独显，用核显进行硬编码或许是一个不错的选择，因为这样就不会和游戏或其他大程序争夺资源。（前提是大程序不使用核显）\n音频编码器：默认（FFmpeg AAC）\n音轨：决定了将会保存哪些音轨，即使有些音轨是空的也会保存。搭配混音器设置。默认所有音源都映射到了所有音轨上，每个音轨都混合了所有音源。\n重新缩放输出：将会缩放到指定分辨率。\n自定义混流器设置：自定义混流器设置。\n自动分割文件：会每隔自定义设置就截断视频并新创建视频文件，也可手动分割。\n编码器设置 速率控制：决定用什么策略来分配码率（有一些是某些视频编码器没有的）\n推荐CFR\u0026gt;CQP\u0026gt;VBR\u0026gt;ABR\u0026gt;CBR\nCBR 固定每秒码率，每秒都会尽量分配一致的码率，不推荐在录制时使用这个，原因是在复杂的环境会导致画面模糊，简单的环境会导致浪费磁盘空间。推荐12000kbps（1080p）。 码率：8192kbps=8.192mbps=1mb/s\nABR 固定平均码率，会尽量把平均码率稳定到一定的值，它只是预测，不太能保证稳定到了，一般，它会在复杂的环境给多码率，简单的环境给少码率，而且会因为当前平均码率来做质量偏移。 码率：8192kbps=8.192mbps=1mb/s\nVBR 可变码率，尽量让每秒的码率不超过目标码率而且在简单环境给少一些码率，仍然，在复杂度超过负荷的环境下，它还是会糊，建议12000kbps（1080p），调高一点目标码率就可以解决问题，但是会产生新问题，你还是会增加磁盘开销。 码率：目标码率，8192kbps=8.192mbps=1mb/s 最大码率：码率不会超过这个值，但实际上码率只会最高高于目标码率大约20%，所以此项设再高也没用。\nCQP 固定每帧cq，cq难理解，说成画质就好理解了，一般，CQ级别增减6会导致视频大小增加1倍或减少一半，推荐在18-28，18的画质看起来就无损了，设为0代表无损编码。它不会管每秒到底花了多少磁盘空间，而是注意画质的好坏，28是普遍能接受的最低值。 如果你可以使用CFR，就不要使用CQP。 CQ 级别：CQ 级别。\n无损 无损编码，但是CQP 0 似乎兼容性更高。\nCFR 更人性化的CQP，在高动态画面降低CQ级别来节省磁盘空间（因为高动态画面人眼难以捕捉细节，更高的画质可能作用不大，浪费磁盘空间），而低动态画面则会增加CQ级别来提高细节保留（因为低动态画面人眼将聚焦细节，更低的画质会令人厌烦，并且低动态画面即使保留细节也不会增加太多磁盘占用） CFR：你可以理解为目标CQ级别。\n关键帧间隔：每多少秒才存储一整张图片，高的值可以在低动态画面降低码率消耗，代价是播放时需要强劲的解码器（假如设为10s，你放pr里可能会卡的离谱，因为需要计算最多10秒的画面才能输出现在的画面），低的值可以在低配置设备上更流畅的播放和编辑，代价是磁盘遭老罪。最高设为10，推荐2-8\nCPU使用预设（x264）：折磨cpu的指标（不是），更上面的占用更低的cpu和带来更低的编码质量，反之则反，不过受限于边际效应，更下面的预设不一定能带来更好的效果，比如你设为placebo，跟veryslow对比，质量提升大约1%，但是需要多100%的cpu性能。能设多低设多低吧（列表中的低）。\n预设：和上述大差不差，有时候是用显卡罢了。\n配置（Profile）：\n配置：main10有更高压缩率。\n调节：选高质量。\n最大B帧：最多B帧\n前向考虑：开启动态b帧，只需要足够多的b帧就可以完善画面，关闭则总为最大。\n心理视觉调整：开\n二次编码：能选多高选多高，但是预设更优先增加。\n微调（Tune）：\nfilm 电影、真人类型。\nanimation 动画。\ngrain 需要保留大量的grain时用。\nstillimage 静态图像编码时使用。\npsnr 为提高psnr做了优化的参数。\nssim 为提高ssim做了优化的参数。\nfastdecode 可以快速解码的参数。\nzerolatency 零延迟，用在需要非常低的延迟的情况下，比如电视电话会议的编码。\nx264选项（用空格分割）：高级选项，可自定义x264参数。https://ffmpeg.org/\n**FFmpeg 选项：**高级选项，可自定义FFmpeg参数。https://ffmpeg.org/\n作者：寒琴庭霜wawdili https://www.bilibili.com/read/cv28371449/ 出处：bilibili\n","date":"2023-12-11T11:07:04+08:00","permalink":"https://ZHY2019302848.github.io/p/obs/","title":"Obs"},{"content":"清洗UrbanBIS数据 把Longhua Qingdao Wuhu Yingrenshi Yuehai五个城市的可用房屋mesh数据以及其对应的材质图片提取出来\n现在有可用的房屋mesh数据862个。\n本周PLAN 删除无用纹理图片 写一个py脚本，先存储每一个mtl文件中提到的jpg文件，之后在wenli文件夹下搜索，只要是没有使用过的图片就删除,去除无用数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import os import re def extract_texture_names(mtl_path): texture_names = set() with open(mtl_path, \u0026#39;r\u0026#39;) as mtl_file: for line in mtl_file: match = re.match(r\u0026#39;map_Kd\\s+\\.\\.\\/\\.\\.\\/wenli\\/([^\\s]+\\.jpg)\u0026#39;, line) if match: texture_name = match.group(1) texture_names.add(texture_name) return texture_names def delete_unused_textures(wenli_dir, used_texture_names): for texture_file in os.listdir(wenli_dir): if texture_file.lower().endswith(\u0026#39;.jpg\u0026#39;) and texture_file not in used_texture_names: texture_path = os.path.join(wenli_dir, texture_file) # 删除不需要的纹理 os.remove(texture_path) print(f\u0026#34;Deleted: {texture_path}\u0026#34;) def process_building_folder(root_dir, building_folder): building_path = os.path.join(root_dir, building_folder, \u0026#39;building\u0026#39;) all_used_texture_names = set() for sub_building_folder in os.listdir(building_path): sub_building_folder_path = os.path.join(building_path, sub_building_folder) if os.path.isdir(sub_building_folder_path): mtl_files_path = sub_building_folder_path used_texture_names = set() # 针对mtl文件做图片名称提取 for mtl_file_name in os.listdir(mtl_files_path): if mtl_file_name.endswith(\u0026#34;.mtl\u0026#34;): mtl_file_path = os.path.join(mtl_files_path, mtl_file_name) used_texture_names.update(extract_texture_names(mtl_file_path)) # 合并之前提取过的 all_used_texture_names.update(used_texture_names) print(f\u0026#34;Used texture names in {building_folder}: {all_used_texture_names}\u0026#34;) # 删除 wenli_dir = os.path.join(root_dir, building_folder, \u0026#39;wenli\u0026#39;) delete_unused_textures(wenli_dir, all_used_texture_names) if __name__ == \u0026#34;__main__\u0026#34;: root_directory = \u0026#34;E:/UrbanBIS_useful/mesh/Yuehai\u0026#34; # 遍历文件夹 for building_folder in os.listdir(root_directory): building_folder_path = os.path.join(root_directory, building_folder) if os.path.isdir(building_folder_path): process_building_folder(root_directory, building_folder) 对建筑进行划分 在进行划分和第二次清洗后，得到652可用房屋mesh文件\n给之前的Building3D建筑加纹理 :（ 要在原有数据集的基础上添加纹理，最好的方法应该是在原来obj文件的基础上添加纹理坐标，然后使用外部mtl加载纹理图片，这里有必要学习一下，一个obj文件是如何加载一个外部纹理图片的。\n想要构成一个面至少需要三个顶点。\n下面是一个标准正方体的obj文件格式：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 v 1.000000 1.000000 -1.000000 v 1.000000 -1.000000 -1.000000 v 1.000000 1.000000 1.000000 v 1.000000 -1.000000 1.000000 v -1.000000 1.000000 -1.000000 v -1.000000 -1.000000 -1.000000 v -1.000000 1.000000 1.000000 v -1.000000 -1.000000 1.000000 f 1 5 7 3 f 4 3 7 8 f 8 7 5 6 f 6 2 4 8 f 2 1 3 4 f 6 5 1 2 现在这个obj没有法线坐标也没有纹理坐标，仅仅作为展示一个模型如何通过连点成面构成一个简单的正方体mesh,可以看到每一个面都是由四个点连线得到，总共构成六个面。\n加上顶点法线坐标，用来表示面的朝向\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 v 1.000000 1.000000 -1.000000 v 1.000000 -1.000000 -1.000000 v 1.000000 1.000000 1.000000 v 1.000000 -1.000000 1.000000 v -1.000000 1.000000 -1.000000 v -1.000000 -1.000000 -1.000000 v -1.000000 1.000000 1.000000 v -1.000000 -1.000000 1.000000 vn -0.0000 1.0000 -0.0000 vn -0.0000 -0.0000 1.0000 vn -1.0000 -0.0000 -0.0000 vn -0.0000 -1.0000 -0.0000 vn 1.0000 -0.0000 -0.0000 vn -0.0000 -0.0000 -1.0000 f 1//1 5//1 7//1 3//1 f 4//2 3//2 7//2 8//2 f 8//3 7//3 5//3 6//3 f 6//4 2//4 4//4 8//4 f 2//5 1//5 3//5 4//5 f 6//6 5//6 1//6 2//6 在表示面的那一列，如“1//1”中间空出来的部分是留给纹理坐标。\n为了能够把纹理映射(Map)到一个面上，我们需要指定一个面的每个顶点各自对应纹理的哪个部分。这样每个顶点就会关联着一个纹理坐标(Texture Coordinate)，用来标明该从纹理图像的哪个部分采样。之后在图形的其它片段上进行片段插值(Fragment Interpolation)。\n单个纹理坐标只需要处理二维即可，因为纹理图片只需要考虑二维表面上的映射问题。\n像下面图片所示，指定了图片的三个点，之后把三个点分别链接到三个顶点坐标上，就表示这张纹理在哪一个面上 纹理坐标:\n1 2 3 vt 0.0f 0.0f # 左下角 vt 1.0f 0.0f # 右下角 vt 0.5f 1.0f # 上中 知道以上的内容后，准备一张纹理图片(texture.jpg),然后把这张纹理图完整的bia到正方体的每一个面上，obj内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 mtllib untitled.mtl v 1.000000 1.000000 -1.000000 v 1.000000 -1.000000 -1.000000 v 1.000000 1.000000 1.000000 v 1.000000 -1.000000 1.000000 v -1.000000 1.000000 -1.000000 v -1.000000 -1.000000 -1.000000 v -1.000000 1.000000 1.000000 v -1.000000 -1.000000 1.000000 vn -0.0000 1.0000 -0.0000 vn -0.0000 -0.0000 1.0000 vn -1.0000 -0.0000 -0.0000 vn -0.0000 -1.0000 -0.0000 vn 1.0000 -0.0000 -0.0000 vn -0.0000 -0.0000 -1.0000 #这四个点代表整张图片全部用到 vt 0 0 vt 1 0 vt 1 1 vt 0 1 f 1/1/1 5/2/1 7/3/1 3/4/1 f 4/1/2 3/2/2 7/3/2 8/4/2 f 8/1/3 7/2/3 5/3/3 6/4/3 f 6/1/4 2/2/4 4/3/4 8/4/4 f 2/1/5 1/2/5 3/3/5 4/4/5 f 6/1/6 5/2/6 1/3/6 2/4/6 最后效果如下图所示：\n一些常用命令总结 powershell终端下运行该命令可以创建多个序列号文件夹(windows)\n1 1..100 | ForEach-Object {New-Item -ItemType Directory -Path (\u0026#34;building$_\u0026#34;)} 统计一个文件夹下特定文件名后缀的文件数量,以obj文件举例\nwindows（cmd）\n1 dir /b /s /a-d \u0026#34;C:\\Path\\To\\Your\\Folder\\*.obj\u0026#34; | find /c /v \u0026#34;\u0026#34; linux\n1 find /path/to/your/folder -type f -name \u0026#34;*.obj\u0026#34; | wc -l ","date":"2023-11-28T11:02:09+08:00","permalink":"https://ZHY2019302848.github.io/p/mesh/","title":"Mesh"},{"content":"多目标追踪(Multi-Object-Tracking) 研究背景及意义 多目标跟踪(Multiple object tracking，MOT)的主要任务是从给定视频中输出所有目标的运动轨迹，并维持各目标的身份信息(Identity，ID)。其中，跟踪目标可以是行人、车辆或其他对象。 视频监控系统的普及： 随着视频监控系统在公共场所、交通系统、工业领域等的广泛应用，需要自动化地对视频中的多个目标进行实时监测和追踪。 自动驾驶技术的发展： 在自动驾驶领域，多目标追踪对于车辆和行人的准确追踪至关重要，以确保车辆能够安全行驶并适应复杂的交通环境。 智能交通系统的需求： 对于城市交通管理和规划来说，多目标追踪能够提供实时的交通流量信息、行人流动模式等，帮助改善交通系统的效率和安全性。 人机交互与辅助技术： 在智能家居、虚拟现实等领域，多目标追踪有助于提供更智能的人机交互和环境感知。 人工智能与机器学习： 多目标追踪是人工智能和机器学习领域的一个典型问题，解决这一问题涉及到对图像处理、目标检测、时空建模等多个方面的研究，有助于推动计算机视觉和机器学习的发展。 研究难点 然而，在复杂场景下进行鲁棒跟踪仍是当前的研究难点，主要体现在以下几个个方面：\n目标变化： 在实际场景中，目标可能经历尺寸、形状、外观等多方面的变化，如遮挡、部分遮挡、光照变化等。这种变化使得目标的外观难以预测，需要算法具有强大的适应性和泛化能力。 复杂运动： 目标在运动中可能出现快速变化、突然停止、加速等复杂运动模式。这使得算法需要能够准确地捕捉目标的运动状态，并且在快速变化的情况下保持稳定的跟踪性能。 遮挡和交叉： 多目标追踪中，目标之间可能存在遮挡和交叉的情况。这种情况下，算法需要能够正确地分离和跟踪各个目标，避免混淆。 非刚性目标： 一些目标可能是非刚性的，比如液体、布料等。这种类型的目标形状难以建模，因此需要算法能够处理非刚性目标的变形和运动。 环境干扰： 多目标追踪系统通常在真实世界环境中运行，受到各种环境干扰的影响，如雨、雪、光照变化、阴影等。这些因素可能导致传感器数据的不稳定性，增加了目标跟踪的难度。 大规模目标集合： 在一些应用场景中，可能需要同时跟踪大量目标，如交通监控、人群管理等。这会导致计算复杂度增加，需要高效的多目标跟踪算法来处理大规模目标集合。 实时性要求： 在一些实时应用中，如自动驾驶、视频监控等，对目标跟踪算法的实时性要求非常高。算法需要在保持高准确性的同时，具备足够的计算效率，以满足实时处理的需求。 评价指标 经典方法 MOT算法分类 基于SDE(Separate Detectionand Embedding)范式的方法 离线跟踪算法 离线跟踪可以看成是一个全局优化问题，给定所有视频帧的检测结果，将属于同一目标的检测结果全局关联到一条轨迹中。\n离线跟踪的关键是找到全局最优解。连续能量最小化是一种常用的全局优化方法，旨在将数据关联和轨迹估计整合到能量函数中，并通过构建运动模型来约束轨迹。另一种常用的全局优化策略是将MOT任务建模为一个图模型，其中每个顶点表示一个检测目标，顶点间的边表示目标间的相似性，然后通过匈牙利算法或贪婪算法确定各顶点的匹配关系。基于图模型的方法有网络流(Network Flow，NF)、条件随机场(CRF)、最小代价子图多切(Minimum Cost Subgraph Multicut，MC SM)和最大加权独立集(Maximum-Weight Independent Set，MWIS)等。\n由于在跟踪过程中可利用更多帧图像的信息，离线方法通常比在线方法具有更高的跟踪准确性和鲁棒性，但其计算量开销更高且实际应用范围相比在线方法较小。\n在线跟踪算法 由于在线跟踪方法具有不依靠未来信息的特点，更契合实际需求，因此在线的跟踪算法成为如今的研究主流。在线跟踪方法通常按时间顺序逐帧关联目标，因此在线跟踪也被称为顺序跟踪。当前的在线跟踪方法常基于目标的运动和外观特征关联目标。早期的研究主要通过构建运动模型，基于目标的运动特征来跟踪目标。随后，受益于神经网络强大的特征提取能力，基于外观特征的跟踪算法吸引了广泛的关注。而为了进一步提升算法在各种复杂的场景下的跟踪准确性，结合运动和外观特征的MOT算法成为了当今的研究热点。\n可以分为三种（做一个图，后面跟上这些方法的优缺点）\n基于运动特征的方法 优点：有效应对短时间的遮挡且缓解了相似目标对模型的干扰 缺点：由于外观特征的缺失，跟踪性能衰退明显 基于外观特征的方法 优点：跟踪能力更强，对目标尺度变换的鲁棒性更高 缺点：有相似目标干扰的场景下容易发生跟踪框漂移等错误 结合运动和外观特征的方法 优点：跟踪准确性较高，应对复杂场景下的各种挑战具有更强的鲁棒性 缺点：网络复杂度较高且计算量相对较大，跟踪速度较慢，难以达到实时跟踪的要求 基于JDE(Joint Detection and Embedding)范式的方法 SDE的方法在跟踪过程中先后推理了目标检测和特征提取两个计算量较大的深度网络，这种高昂的计算开销限制了模型的跟踪速度，通过使目标检测和特征提取两个关键任务共享大量特征，即将两个任务融合到一个网格中去，JDE范式可以显著减少算法的计算量。 优化的具体三点\n联合检测和嵌入 JDE采用联合检测和嵌入的方式，通过一个统一的深度学习模型，同时执行目标检测和目标嵌入。 JDE基于深度卷积神经网络（CNN），包含目标检测分支和目标嵌入分支，两者共享底层卷积层，以促使特征提取得到充分共享和协同训练。 训练时采用联合训练策略，同时优化目标检测分支和目标嵌入分支的损失函数。损失函数包括目标检测的定位损失、分类损失，以及目标嵌入的三元损失等，平衡了目标检测和嵌入学习两个任务。 基于JDT(Joint Detectionand Tracking)范式的方法 联合检测和跟踪在单个网格中完成三个子任务进行算法优化 当前JDT范式的算法主要分为基于孪生网路的方法和Transformer的方法 基于孪生网络的方法 基本原理：孪生网络是标准CNN的一种变体。如图所示，基于孪生网络的方法通过两个共享权重的卷积层提取不同视频帧图像中目标的特征，结合不同图像信息学习目标更具判别性的特征。 训练过程: 在训练孪生网络时，通常使用孪生三元损失（Siamese Triplet Loss）来优化网络参数。损失函数包括一个锚定样本、一个正样本和一个负样本，通过最小化锚定样本与正样本的距离同时最大化锚定样本与负样本的距离，使得相似的目标在嵌入空间中更加接近。 目标跟踪过程：在跟踪时，通过比较当前帧目标图像和前一帧目标图像的特征表示相似性，判断目标是否相同。一些孪生网络方法还采用在线学习的策略，通过不断地更新孪生网络的权重，适应目标外观的变化，提高跟踪的鲁棒性。 Transformer方法 基本原理:Transformer是一种基于自注意力机制的神经网络结构，广泛应用于序列建模任务。在目标跟踪中，可以将目标的位置信息作为序列输入，并使用Transformer模型来捕捉目标之间的关系。 自注意力机制：Transformer使用自注意力机制来动态地捕获输入序列中各元素之间的关系，使得模型能够关注不同位置的信息，这其中的输入为目标的位置信息和特征，将二者嵌入组成序列。 目标跟踪过程：Transformer可以输出每个目标的位置信息以及其他可能的特征，通过比较输出信息的相似性进行目标匹配，能够更好地处理目标之间的复杂关系和变化 目标追踪方法 论文复现 总结 近年来，基于深度学习的MOT技术迅速发展，模型的跟踪性能取得了显著的提升，目前已有越来越多的技术被应用到MOT任务上，但目前还有许多值得探索的研究方向。\n无监督MOT：当前的MOT算法大多是基于监督学习，然而MOT数据集的标注需要逐帧寻找不同图像间的相同目标，需花费巨大的时间和经济成本。设计基于无监督学习的MOT算法有助于减少人工标注数据的开销，然而由于缺乏对跟踪目标的先验知识，无监督MOT任务具有很大的挑战性。 目标间交互关系：通过对多个目标间的交互关系建模，可增强拥挤场景下模型对各目标的判别能力，然而当前算法对于目标间交互关系的探索依然较少。在今后的研究工作中，可采用Transformer或图神经网络对目标间的交互关系进行建模，从而进一步提升MOT算法在高峰时段的地铁站和节假日的旅游景点等极端拥挤场景下的跟踪鲁棒性。 跟踪促进检测：当前的MOT算法跟踪性能依赖于检测算法，然而目前的MOT算法通常单独执行检测算法，并未探索目标在先前时刻的信息。充分利用目标的时空信息，将目标在过去时刻的运动和外观等特征传递到当前帧，有助于提升模型在执行交通车辆跟踪和赛场运动员行为分析等存在大量遮挡和运动模糊的跟踪任务时的跟踪性能。 ","date":"2023-11-23T18:31:02+08:00","permalink":"https://ZHY2019302848.github.io/p/%E5%AD%A6%E4%B9%A0/","title":"MOT"},{"content":"Wonder3D环境配置 项目仓库\n项目论文\n准备系统环境 系统为ubuntu18.04以上系统，anaconda环境装好，cuda版本为11.8或11.7，测试用12.2会报错，gcc版本8以上，python3.8以上，显卡确保显存足够，我先后在2080ti和titan显卡上均测试过，cuda版本为11.3和11.2，都会报cuda和pytorch版本不匹配的错，最后在4090上成功运行，所以一个准确合适的环境是重要前提。\n安装环境 只要上一步系统环境准备完好，接下来依照官方教程即可\n1 2 conda create -n wonder3d python=3.10 #这里用3.10是怕再有奇怪的报错，所以把版本调高 conda activate wonder3d 这两步结束后，先查看python和pip的路径是conda虚拟环境，用which python和which pip查询，如果返回时虚拟环境的路径，可以跳过这一步骤，如果不是 ，显示为local路径，可以先退出现在的base环境，然后在换回来wonder3d环境\n1 2 3 conda deactivate #退出wonder3d conda deactivate #退出base conda activate wonder3d #重新激活环境 下面是项目下载和必要环境的安装\n1 2 3 4 git clone https://github.com/xxlong0/Wonder3D #这里如果速度慢可以参考上一篇\u0026#34;服务器网络问题解决方案\u0026#34;，但这个只能用于大学教研室或是公司本地服务器 cd Wonder3D pip install -r requirements.txt #这要下很久，可以去泡杯茶 pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch #这里很容易出问题，大部分问题都可以在\u0026#34;准备系统环境\u0026#34;中解决 安装完后看一下自己torch是否安装成功\n1 python 1 2 3 import torch print(torch.__verison__)#查看版本号是否对应后退出即可 exit() 运行项目 先下载checkpoint文件复制到指定目录下\n1 2 3 4 5 Wonder3D |-- ckpts |-- unet |-- scheduler.bin ... 之后可以直接运行命令看一下效果，代码会默认跑一只猫头鹰\n1 bash run_test.sh 同理，此处出现错误请返回系统环境和安装环境这两个步骤查看是否出错，如果按照错误提示打补丁很可能越补窟窿越大 运行完成后可以在./outputs下查找相关文件，应该6个一一对应的法线图和色彩图\n之后生成mesh obj文件，这里我只试了官方的第一种方法\n1 2 3 4 cd ./instant-nsr-pl bash run.sh output_folder_path scene_name #第二条命令示例为: bash run.sh ../outputs/cropsize-192-cfg3.0 owl 生成完成后可以用ls命令找哪个文件夹是新生成的说明obj文件就存在哪了\n如果想要替换自己的图片，先把图片的背景去掉，在线网站就可以完成，然后去找/wonder/configs/mvdiffusion-joint-ortho-6views.yaml，修改如下内容即可 尝试其他图片 我用该方法尝试了下最近师姐让找的car图片，因为该项目训练时没有车的训练模型，所以图生3D的效果不是十分理想。\ncar1 car2 car3 总结 锻炼了自己的环境配置功力，还有在项目介绍看到可以使用正交相机来完成对obj色彩的显示，原文如下：\nOur generated normals and color images are defined in orthographic views, so the reconstructed mesh is also in orthographic camera space. If you use MeshLab to view the meshes\n这个可以帮到我最近在研究的对对房屋mesh涂上迷彩的问题。\n","date":"2023-11-15T19:37:07+08:00","image":"https://ZHY2019302848.github.io/p/wonder3d%E9%A1%B9%E7%9B%AE%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%92%8C%E8%BF%90%E8%A1%8C/fig_teaser_hu5f338f1929158fc1dbd893b6f223493f_1271319_120x120_fill_box_smart1_3.png","permalink":"https://ZHY2019302848.github.io/p/wonder3d%E9%A1%B9%E7%9B%AE%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%92%8C%E8%BF%90%E8%A1%8C/","title":"Wonder3D项目环境配置和运行"},{"content":"对服务器网络问题的总结 最近使用实验室linux服务器跑代码，对其中频繁出现的网络问题烦不胜烦，诸如git和pip问题层出不穷，其中的问题不是简单换一个清华源可以，根源性需要让服务器走中转代理，实现完全性翻墙（该方法仅对内网有用）。\n现在把其中常见的命令总结如下，方便以后快捷使用。\ngit的代理设置 先对clash进行设置\n1 2 3 git config --global https.proxy 127.0.0.1:7890 #端口号参考clash git config --global http.proxy 127.0.0.1:7890 #端口前面的ip地址参考开clash的主机ip 如果报错提示不让写入修改，在最后加入--replace-all，代码如下\n1 2 git config --global https.proxy 127.0.0.1:7890 --replace-all git config --global http.proxy 127.0.0.1:7890 --replace-all 服务器全局设置 考虑到有些工具无法设置代理，所以使用全局代理，但这种方式在针对ICMP协议时会失效，因为使用的代理走的都是会话层，无法影响到ip报文解包的结果。\n代码如下\n1 2 3 export proxy=\u0026#34;http://127.0.0.1:7890\u0026#34; ##ip和端口地址按情况替换 export https_proxy=$proxy export http_proxy=$proxy 效果 可以让torch的安装速度从几百k涨到最高10mb/s。\n备注\n如果无法正常部署博客到github.io，请检查标题里是不是把草稿draft设置为了true，之后再运行如下命令\n1 2 hugo hugo --gc --minify --cleanDestinationDir ","date":"2023-11-10T15:34:23+08:00","image":"https://ZHY2019302848.github.io/p/linux-network/linux_network_hu0f0ae4cf8191179714ea2b2d64d558ac_81456_120x120_fill_q75_box_smart1.jpg","permalink":"https://ZHY2019302848.github.io/p/linux-network/","title":"服务器网络问题解决方案"}]